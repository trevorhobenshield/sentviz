{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85a68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "def view(df_):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "        display(HTML(df_.to_html()))\n",
    "\n",
    "df = pd.read_parquet('yt_transcripts_fastai.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "729b92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17818</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=147</td>\n",
       "      <td>each token what is it you know and I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19237</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4147</td>\n",
       "      <td>and that's called tokenization okay and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19246</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4178</td>\n",
       "      <td>of tokens is called tokenization right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4180</td>\n",
       "      <td>and so a good tokenizer would turn this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19252</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4197</td>\n",
       "      <td>spaces every token is either a single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19256</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4207</td>\n",
       "      <td>probably want to tokenize that piece of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19263</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4226</td>\n",
       "      <td>a tokenizer first AI has a tokenizer in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19293</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4309</td>\n",
       "      <td>will have a special token for unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19355</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=4483</td>\n",
       "      <td>the tokenizer here I'm just bidding on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19817</th>\n",
       "      <td>https://www.youtube.com/watch?v=37sFIak42Sc&amp;t=5836</td>\n",
       "      <td>question about the token icers so you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29572</th>\n",
       "      <td>https://www.youtube.com/watch?v=9C06ZPF8Uuc&amp;t=5392</td>\n",
       "      <td>classification so by the same token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80464</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6253</td>\n",
       "      <td>it starts with a special token xx POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80466</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6258</td>\n",
       "      <td>fast a token the OS is the beginning of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80467</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6261</td>\n",
       "      <td>stream token it basically says this is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80475</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6279</td>\n",
       "      <td>between spaces is a separate token the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80486</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6319</td>\n",
       "      <td>string of 13,000 tokens and then we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80499</th>\n",
       "      <td>https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=6363</td>\n",
       "      <td>13,000 tokens how many batches are there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39693</th>\n",
       "      <td>https://www.youtube.com/watch?v=H3g26EVADgY&amp;t=1669</td>\n",
       "      <td>time we used a tokenization function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39699</th>\n",
       "      <td>https://www.youtube.com/watch?v=H3g26EVADgY&amp;t=1683</td>\n",
       "      <td>character put into a separate token so I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39750</th>\n",
       "      <td>https://www.youtube.com/watch?v=H3g26EVADgY&amp;t=1830</td>\n",
       "      <td>the number of tokens divided by the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39791</th>\n",
       "      <td>https://www.youtube.com/watch?v=H3g26EVADgY&amp;t=1958</td>\n",
       "      <td>approximate okay number of tokens is how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69800</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6167</td>\n",
       "      <td>something called tokenization or does it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69802</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6173</td>\n",
       "      <td>into a standard form of tokens where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69803</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6176</td>\n",
       "      <td>there's basically each token represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69809</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6191</td>\n",
       "      <td>words so tokenization is trying to make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69810</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6194</td>\n",
       "      <td>sure that each each token each each</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69819</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6221</td>\n",
       "      <td>special token so this is tokenization so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69821</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6227</td>\n",
       "      <td>a list of tokenized words you'll also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69824</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6234</td>\n",
       "      <td>that they're separate tokens the next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69826</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6242</td>\n",
       "      <td>list of all of the possible tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69830</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6252</td>\n",
       "      <td>possible token the first ten of them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69846</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=6303</td>\n",
       "      <td>them to datasets tokenize the numerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70098</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=7015</td>\n",
       "      <td>okay okay tokenization how do you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70100</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=7028</td>\n",
       "      <td>san francisco contains two tokens San</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70119</th>\n",
       "      <td>https://www.youtube.com/watch?v=MpZxV6DVsmM&amp;t=7077</td>\n",
       "      <td>each token is literally just a word or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125786</th>\n",
       "      <td>https://www.youtube.com/watch?v=VEG5xT5gAHc&amp;t=929</td>\n",
       "      <td>Python. By the same token at the bottom we've\\ngot some English Patient, Harry Met Sally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126894</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1000</td>\n",
       "      <td>So if we use that tokenizer we created and\\npass in for example this text, you can see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126895</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1006</td>\n",
       "      <td>the way it's tokenized, we get the xx beginning\\nof stream, or beginning of string, beginning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126902</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1054</td>\n",
       "      <td>with just making it standard tokens. So that's\\nthe word tokenizer. The really interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126903</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1061</td>\n",
       "      <td>one is the subword tokenizer. So how, why\\nwould you need a subword tokenizer? Well,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126909</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1096</td>\n",
       "      <td>start of a sentence, and some of its at the\\nend. So you can't really do word tokenization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126910</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1101</td>\n",
       "      <td>to something like Chinese. So instead we use\\nsub word tokenization, which is where we look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126922</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1176</td>\n",
       "      <td>need to be set up before you can use them.\\nIn other words you can't tokenize into sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126925</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1195</td>\n",
       "      <td>word tokenizer. It'll find those commonly\\noccurring groups of letters. So having done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126929</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1220</td>\n",
       "      <td>with a thousand tokens, and it returns this,\\nthis tokenized string. Now this kind of long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126930</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1230</td>\n",
       "      <td>underscore thing is what we replace space\\nwith because now we're using sub word tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126957</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1400</td>\n",
       "      <td>we'll get it done pretty soon. All right.\\nSo after we split it into tokens the next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126958</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1408</td>\n",
       "      <td>thing to do is numericalization. So let's\\ngo back to our word tokenized text, which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126966</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1458</td>\n",
       "      <td>starts with the special tokens and then we\\nstart getting the English tokens in order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126970</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1491</td>\n",
       "      <td>unknown token. So that'll help us avoid having\\na too big embedding matrix. All right, so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126979</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1562</td>\n",
       "      <td>if we tokenize that text it'll convert it\\ninto this, and so let's, let's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126984</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1604</td>\n",
       "      <td>will look at the[...]� etc. Okay? So we've\\ntaken these ninety tokens, and to create a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126991</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1661</td>\n",
       "      <td>there's a few million tokens of IMDb, so a\\nfew million divided by 64 across, it's going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127008</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1772</td>\n",
       "      <td>So if we take those, all the tokens from the\\nfirst 200 movie reviews, and map them through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127009</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1777</td>\n",
       "      <td>our Numericalize object. Right? So now we've\\ngot numericalized versions of all those tokens,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127014</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1817</td>\n",
       "      <td>So if we grab the first of our independent\\nvariables and grab the first few tokens, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127029</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=1923</td>\n",
       "      <td>actually passing in here a class method, and\\nthat's so that we can allow the tokenization,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127116</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=2461</td>\n",
       "      <td>the category. Yes, question? Do the tokenizers\\nuse any tokenization techniques like stemming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127117</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=2471</td>\n",
       "      <td>or lemmatization, or is that an outdated approach?\\nThat would not be a tokenization approach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127144</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=2656</td>\n",
       "      <td>by adding padding. So we're going to add a\\nspecial xx_pad token to every sequence in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127146</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=2672</td>\n",
       "      <td>padding tokens to make this 581 and this 581\\nand this 581 and so forth, and then we can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127285</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3589</td>\n",
       "      <td>together and put a full-stop between them.\\nAs so... Okay? And then you could tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127286</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3597</td>\n",
       "      <td>that by splitting on spaces, and so for example\\nhere's tokens 100 to 110. New number 42, new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127289</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3616</td>\n",
       "      <td>create all the unique tokens, of which there\\nare 30, and then to create a lookup from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127292</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3641</td>\n",
       "      <td>numericalize our tokens by calling word to\\nindex on each one, and so here's our tokens,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127295</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3662</td>\n",
       "      <td>have the details of tokenization in English,\\nyou can do the whole thing in just plain Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127298</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3682</td>\n",
       "      <td>so hopefully that gives you a good sense of\\nreally what's going on with tokenization and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127300</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3693</td>\n",
       "      <td>a language model would be to go through all\\nof our tokens, and let's create a range from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127301</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3699</td>\n",
       "      <td>0 to the length of our tokens minus 4, and\\nevery 3 of them, and so that's going to allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127302</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3705</td>\n",
       "      <td>us to grab three tokens at a time 1 dot 2\\ndot 3 dot 4 dot five dot 6 dot 7 dot 8, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127303</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3716</td>\n",
       "      <td>so forth. Right? So here's the first three\\ntokens, and then here's the 4th token, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127304</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3723</td>\n",
       "      <td>here's the second three tokens, and here's\\nthe seventh token, and so forth. So these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127311</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=3770</td>\n",
       "      <td>Right? And so we can just grab the first 80%\\nof the tokens as the training set, the last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127402</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=4364</td>\n",
       "      <td>one token to the next of our language model,\\nso we would expect it to be the same computation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127412</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=4432</td>\n",
       "      <td>average or something. So what I did is, I\\ngrabbed the validation set (so all the tokens),</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127509</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=5116</td>\n",
       "      <td>next three tokens to predict the seventh,\\nand then the next three tokens to predict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126819</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=541</td>\n",
       "      <td>concatenated corpus and turn it into a list\\nof tokens: could be words, could be characters,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126820</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=547</td>\n",
       "      <td>could be substrings. That's called tokenization.\\nAnd then we'll do numericalization, which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126824</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=573</td>\n",
       "      <td>of tokens from our IMDB corpus as an independent\\nvariable, and the same thing offset by one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126833</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=625</td>\n",
       "      <td>Okay so let's start with the first of these,\\nwhich is tokenization. So converting a text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126834</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=633</td>\n",
       "      <td>into a list of words or a list of tokens.\\nWhat does that mean? Is a full-stop a token?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126846</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=700</td>\n",
       "      <td>marks as a separate token most of the time.\\nReally interestingly, there are tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126849</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=718</td>\n",
       "      <td>split a sentence into its characters. We're\\ngoing to look at word- and sub-word tokenization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126851</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=727</td>\n",
       "      <td>to create your own character-based tokenizer,\\nso please make sure you do that if you can,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126852</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=732</td>\n",
       "      <td>it'll be a great exercise! So fastai doesn't\\ninvent its own tokenizers. We just provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126854</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=747</td>\n",
       "      <td>tokenizers out there. So you can switch between\\ndifferent tokenizers pretty easily. So let's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126864</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=805</td>\n",
       "      <td>it is. Okay so at the moment at the default\\nEnglish word tokenizer we use is called spaCy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126867</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=824</td>\n",
       "      <td>ahead and say WordTokenizer which will automatically\\nuse fastai�s default word tokenizer, currently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126869</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=838</td>\n",
       "      <td>here) to the tokenizer we just created and\\njust grab the first, since we just created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126870</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=843</td>\n",
       "      <td>a list, that's going to show us, as you can\\nsee, the tokenized version. So you can see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126873</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=865</td>\n",
       "      <td>mark, and so forth. Okay so you can see how\\nit has tokenized this review. Let's look at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126878</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=902</td>\n",
       "      <td>this tokenizer wrapper which provides some\\nadditional functionality to any tokenizer,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126880</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=919</td>\n",
       "      <td>capital �It� has been turned into a lowercase\\n�it� and then a special token xxmaj has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126881</th>\n",
       "      <td>https://www.youtube.com/watch?v=WjnwWeGjZcM&amp;t=926</td>\n",
       "      <td>appeared at the front. Everything starting\\nwith xx is a special fastai token, and this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32680</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=5979</td>\n",
       "      <td>of tokens token is basically like a word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32685</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=5988</td>\n",
       "      <td>that's called tokenization in NLP NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32689</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=5998</td>\n",
       "      <td>when we're doing tokenization is here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32690</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6001</td>\n",
       "      <td>I've I've tokenized that review and then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32695</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6018</td>\n",
       "      <td>become one token right where else lots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32697</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6023</td>\n",
       "      <td>tokens so like a good tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32706</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6049</td>\n",
       "      <td>the best tokenizer I know and so past AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32709</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6057</td>\n",
       "      <td>example of tokenization alright so what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32717</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6078</td>\n",
       "      <td>want to tokenize it with the function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32718</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6080</td>\n",
       "      <td>called Spacey tokenize okay so it hasn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32771</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6216</td>\n",
       "      <td>them up into sentences of 70 tokens or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32814</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6352</td>\n",
       "      <td>generally tokenization is is what we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33018</th>\n",
       "      <td>https://www.youtube.com/watch?v=gbceqO8PpBg&amp;t=6993</td>\n",
       "      <td>tokens NT is the number of tokens that's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50002</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=1683</td>\n",
       "      <td>thing we need to do is tokenization so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50010</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=1708</td>\n",
       "      <td>full stop to be a token and so forth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50011</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=1712</td>\n",
       "      <td>right so tokenization is something that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50032</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=1771</td>\n",
       "      <td>there's not more weird tokens in there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50116</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2010</td>\n",
       "      <td>and so by the same token I've got this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50122</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2024</td>\n",
       "      <td>here token followed by the number of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50126</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2034</td>\n",
       "      <td>we tokenize it and we tokenize it by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50140</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2077</td>\n",
       "      <td>is a list of things to tokenize which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50141</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2082</td>\n",
       "      <td>each part of that list will be tokenized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50166</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2145</td>\n",
       "      <td>beginning of stream token beginning of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50167</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2147</td>\n",
       "      <td>field number one token here's the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50170</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2155</td>\n",
       "      <td>separate token you'll see there's a few</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50191</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2218</td>\n",
       "      <td>unique token to mean the next thing is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50194</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2227</td>\n",
       "      <td>lowercase it's just one token and then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50200</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2241</td>\n",
       "      <td>put in a special token for the next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50225</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2303</td>\n",
       "      <td>the tokens that we call the vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50236</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2334</td>\n",
       "      <td>want every unique token in our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50264</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2402</td>\n",
       "      <td>this is just the list of tokens unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50266</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=2409</td>\n",
       "      <td>two more tokens a token for unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50904</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=4231</td>\n",
       "      <td>it needs to know the number of tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51377</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=5562</td>\n",
       "      <td>get all again save those tokens again we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52016</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7383</td>\n",
       "      <td>like tokenize every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52061</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7516</td>\n",
       "      <td>you don't have to tokenize words instead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52062</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7522</td>\n",
       "      <td>of tokenizing words you can tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52066</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7534</td>\n",
       "      <td>tokenized his on supervised tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52067</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7539</td>\n",
       "      <td>could be tokenized as token either right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52075</th>\n",
       "      <td>https://www.youtube.com/watch?v=h5Tz7gZT9Fo&amp;t=7566</td>\n",
       "      <td>using word level tokenization not quite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105281</th>\n",
       "      <td>https://www.youtube.com/watch?v=hPQKzsjTyyQ&amp;t=2474</td>\n",
       "      <td>processing texts to tokenize them and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70645</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1006</td>\n",
       "      <td>the unique tokens that appear here and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70648</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1014</td>\n",
       "      <td>big list of unique possible tokens is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70651</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1021</td>\n",
       "      <td>replace the tokens with the ID of where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70652</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1027</td>\n",
       "      <td>is that token in the vocab okay and that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70664</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1067</td>\n",
       "      <td>UNK that's an unknown token so when you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70665</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1072</td>\n",
       "      <td>see those unknown tokens it just means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70677</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1104</td>\n",
       "      <td>cased and a token called xx cap will get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70769</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=1336</td>\n",
       "      <td>to tokenize and numerical s so since it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71360</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=2954</td>\n",
       "      <td>combine an LP tokenize data with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=3876</td>\n",
       "      <td>converts the the tokens into a set of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70460</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=543</td>\n",
       "      <td>billion tokens all right so we've got a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70639</th>\n",
       "      <td>https://www.youtube.com/watch?v=qqt3aMPB81c&amp;t=987</td>\n",
       "      <td>from its you know get its own token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52473</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1004</td>\n",
       "      <td>because the the number of tokens in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52475</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1010</td>\n",
       "      <td>length as the number of tokens in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52495</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1055</td>\n",
       "      <td>length output where the tokens in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52497</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1061</td>\n",
       "      <td>order you know specific tokens in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52660</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1497</td>\n",
       "      <td>questions and then we tokenize the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52666</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1511</td>\n",
       "      <td>by default the tokenizer that we have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52668</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1515</td>\n",
       "      <td>around the Spacey tokenizer which is a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52669</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1518</td>\n",
       "      <td>fantastic tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52743</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1697</td>\n",
       "      <td>english tokenizer for a french sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52748</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1711</td>\n",
       "      <td>you know use the right tokenize of your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52754</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1727</td>\n",
       "      <td>of a tokenizer in the same way so we've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52759</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1738</td>\n",
       "      <td>units and so when I say tokenize if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52768</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1764</td>\n",
       "      <td>so have you tokenized to it will save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52770</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1767</td>\n",
       "      <td>step after we create tokens is to turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52779</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1791</td>\n",
       "      <td>extra tokens for beginning of stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52785</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1812</td>\n",
       "      <td>every token into an ID by putting it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52820</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=1898</td>\n",
       "      <td>definitely noticed with tokenization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52954</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=2244</td>\n",
       "      <td>there we've got our tokenized numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53129</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=2680</td>\n",
       "      <td>start because I want that final token to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53188</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=2826</td>\n",
       "      <td>going to take our sequence of tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53353</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3240</td>\n",
       "      <td>special token for uppercase not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53355</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3245</td>\n",
       "      <td>it's not token Tyvek it's not token text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53362</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3260</td>\n",
       "      <td>find some which would being tokenized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53363</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3262</td>\n",
       "      <td>the same way we tokenize that's okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53559</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3815</td>\n",
       "      <td>okay so the beginning of stream token is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53561</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3824</td>\n",
       "      <td>with a beginning of stream token which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53569</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3841</td>\n",
       "      <td>vector for the beginning a stream token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53572</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=3848</td>\n",
       "      <td>beginning of stream token we stick that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53758</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=4392</td>\n",
       "      <td>is that some of the tokens that are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53877</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=4726</td>\n",
       "      <td>them each token in the opposing order so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52465</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=984</td>\n",
       "      <td>of tokens where that sequence of tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52470</th>\n",
       "      <td>https://www.youtube.com/watch?v=tY0n9OT5_nA&amp;t=996</td>\n",
       "      <td>model had multiple tokens for every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109245</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5077</td>\n",
       "      <td>we need to tokenize and you miracle eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109247</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5083</td>\n",
       "      <td>you Spacey for tokenizing and we do a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109248</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5088</td>\n",
       "      <td>few things as we tokenize one thing we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5096</td>\n",
       "      <td>before tokenization so for example if we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109256</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5110</td>\n",
       "      <td>space then we have these special tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109259</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5122</td>\n",
       "      <td>them mainly and these different tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109271</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5157</td>\n",
       "      <td>the TK rep special token so this means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109272</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5162</td>\n",
       "      <td>that there was a repeating token where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109281</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5184</td>\n",
       "      <td>list of special tokens so for example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109291</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5213</td>\n",
       "      <td>marks as one token and so now you have a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109301</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5238</td>\n",
       "      <td>is just three tokens where it can learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109314</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5271</td>\n",
       "      <td>28 exclamation marks into 28 tokens in a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109343</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5340</td>\n",
       "      <td>happen after tokenization because</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109348</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5353</td>\n",
       "      <td>list of tokens why do we do that these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109351</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5365</td>\n",
       "      <td>like an end of stream character token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109360</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5384</td>\n",
       "      <td>new token then we're talking about the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109364</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5397</td>\n",
       "      <td>you have the tokens in place to allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109366</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5402</td>\n",
       "      <td>things are happening tokenization is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109385</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5448</td>\n",
       "      <td>every chunk around the tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109394</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5473</td>\n",
       "      <td>of text and let's try tokenizing and so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109396</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5478</td>\n",
       "      <td>stream did int so int is a token comma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109397</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5483</td>\n",
       "      <td>is a token xx match Darwin so that was a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109414</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5533</td>\n",
       "      <td>so we can tokenize numerical eyes run up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109435</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5594</td>\n",
       "      <td>batches which is just all the tokens for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109553</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=5926</td>\n",
       "      <td>be passing a thousand tokens at a time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109600</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6048</td>\n",
       "      <td>adding a bunch of padding tokens so we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109601</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6050</td>\n",
       "      <td>just pick some arbitrary token but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109602</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6052</td>\n",
       "      <td>you're going to tell PI torch this token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109610</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6071</td>\n",
       "      <td>going to end up with 1980 padding tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109614</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6080</td>\n",
       "      <td>all these padding tokens we don't want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109667</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=6228</td>\n",
       "      <td>of numerical errors tokenize documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110095</th>\n",
       "      <td>https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;t=7424</td>\n",
       "      <td>split into trained and valid tokenize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "term = ' token'\n",
    "flags = re.I\n",
    "temp = df.query('text.str.contains(@term, regex=True, flags=@flags)', engine='python').drop_duplicates()\n",
    "view(temp.sort_values('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "222fc32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=WjnwWeGjZcM\n",
      "https://www.youtube.com/watch?v=h5Tz7gZT9Fo\n",
      "https://www.youtube.com/watch?v=MpZxV6DVsmM\n",
      "https://www.youtube.com/watch?v=gbceqO8PpBg\n",
      "https://www.youtube.com/watch?v=tY0n9OT5_nA\n",
      "https://www.youtube.com/watch?v=9spwoDYwW_I\n",
      "https://www.youtube.com/watch?v=hPQKzsjTyyQ\n",
      "https://www.youtube.com/watch?v=vnOpEwmtFJ8\n",
      "https://www.youtube.com/watch?v=H3g26EVADgY\n",
      "https://www.youtube.com/watch?v=qqt3aMPB81c\n",
      "https://www.youtube.com/watch?v=VEG5xT5gAHc\n",
      "https://www.youtube.com/watch?v=9C06ZPF8Uuc\n",
      "https://www.youtube.com/watch?v=37sFIak42Sc\n"
     ]
    }
   ],
   "source": [
    "# sample = temp.iloc[0:2,1].to_list()\n",
    "# sample\n",
    "temp['x'] = temp['url'].str.replace('&t=\\d+','',regex=True)\n",
    "[print(x) for x in set(temp['x'])];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
